{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmzMfUPMJslglKAjJhnG9L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tiru-Kaggundi/Trade_AI/blob/main/ensemble_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhy5NZgTRsKW",
        "outputId": "248a998e-6d3b-4031-8799-2ad22bdcb8f9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIJ8yXXuRddi",
        "outputId": "67cf69a9-9f13-4d23-d1cc-018cfe08cce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joined OOF rows: 881118 | models: ['xgb_tweedie', 'xgb_log1p', 'lgbm_rmse', 'lgbm_tweedie']\n",
            "Start sMAPE: 0.8644095116242546\n",
            "Optimal weights: {'xgb_tweedie': np.float64(0.4444444444444445), 'xgb_log1p': np.float64(0.16666666666666666), 'lgbm_rmse': np.float64(0.2777777777777778), 'lgbm_tweedie': np.float64(0.11111111111111112)} | OOF sMAPE: 0.8627073108661023\n",
            "Saved: /content/drive/MyDrive/ai4trade/logs/ensemble_weights_trees_h2.json\n"
          ]
        }
      ],
      "source": [
        "# === Optimize ensemble weights on OOF (trees only) — robust to column names ===\n",
        "OOF_DIR = \"/content/drive/MyDrive/ai4trade/predictions/oof\"\n",
        "\n",
        "paths = {\n",
        "    \"xgb_tweedie\":  f\"{OOF_DIR}/xgb_tweedie_oof_xgb_tweedie_h2_20251024_1438_p1.5.parquet\",\n",
        "    \"xgb_log1p\":    f\"{OOF_DIR}/xgb_log1p_oof.parquet\",\n",
        "    \"lgbm_rmse\":    f\"{OOF_DIR}/lgbm_rmse_oof.parquet\",\n",
        "    \"lgbm_tweedie\": f\"{OOF_DIR}/lgbm_tweedie_oof_run_h2_20251023_165145.parquet\",\n",
        "}\n",
        "\n",
        "import pandas as pd, numpy as np, os, json, re\n",
        "\n",
        "KEYS = ['origin','destination','hs6','trade_flow','month']\n",
        "Y_CANDIDATES = ['y','y_true','y_target','target','label','value']\n",
        "PRED_CANDIDATES_BASE = ['y_pred','prediction','pred','yhat','y_pred_mean']\n",
        "\n",
        "def detect_truth_col(df):\n",
        "    for c in Y_CANDIDATES:\n",
        "        if c in df.columns: return c\n",
        "    return None\n",
        "\n",
        "def detect_pred_col(df, model_name):\n",
        "    # try exact pattern first (e.g., y_pred_xgb_tweedie)\n",
        "    exact = f\"y_pred_{model_name}\"\n",
        "    if exact in df.columns: return exact\n",
        "    # then common names\n",
        "    for c in PRED_CANDIDATES_BASE:\n",
        "        if c in df.columns: return c\n",
        "    # then any column that starts with 'y_pred'\n",
        "    for c in df.columns:\n",
        "        if str(c).startswith('y_pred'): return c\n",
        "    # last resort: any column containing the model name\n",
        "    for c in df.columns:\n",
        "        if model_name in str(c): return c\n",
        "    return None\n",
        "\n",
        "def load_oof(name, path):\n",
        "    df = pd.read_parquet(path)\n",
        "    # keys present?\n",
        "    missing = set(KEYS) - set(df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"{name}: missing keys {sorted(missing)} in {path}\")\n",
        "    # truth column (optional here)\n",
        "    ycol = detect_truth_col(df)\n",
        "    # pred column (required)\n",
        "    pcol = detect_pred_col(df, name)\n",
        "    if pcol is None:\n",
        "        raise ValueError(f\"{name}: could not find prediction column in {path}. Columns: {df.columns.tolist()}\")\n",
        "    # build minimal frame; if no truth, mark it\n",
        "    cols = KEYS + ([ycol] if ycol else []) + [pcol]\n",
        "    df = df[cols].copy()\n",
        "    if ycol: df = df.rename(columns={ycol:'y'})\n",
        "    df = df.rename(columns={pcol:name})\n",
        "    if 'y' not in df.columns: df['__no_y__'] = 1\n",
        "    return df\n",
        "\n",
        "# Load all model OOFs\n",
        "frames = {n: load_oof(n, p) for n,p in paths.items()}\n",
        "\n",
        "# Choose a truth source (any file that has 'y')\n",
        "truth_src = None\n",
        "for n, df in frames.items():\n",
        "    if 'y' in df.columns:\n",
        "        truth_src = df[KEYS + ['y']].drop_duplicates()\n",
        "        break\n",
        "if truth_src is None:\n",
        "    cols_report = {n: list(df.columns) for n,df in frames.items()}\n",
        "    raise ValueError(f\"No ground-truth column found in any OOF file. \"\n",
        "                     f\"Checked {Y_CANDIDATES}. Columns: {cols_report}\")\n",
        "\n",
        "# Merge predictions onto the truth rows\n",
        "o = truth_src.copy()\n",
        "for n, df in frames.items():\n",
        "    o = o.merge(df[KEYS + [n]], on=KEYS, how='inner')\n",
        "\n",
        "models = list(paths.keys())\n",
        "print(\"Joined OOF rows:\", len(o), \"| models:\", models)\n",
        "\n",
        "def smape_np(y, yhat, eps=1.0):\n",
        "    y = np.asarray(y, float); yhat = np.asarray(yhat, float)\n",
        "    return float(np.mean(2*np.abs(y - yhat) / np.maximum(np.abs(y)+np.abs(yhat), eps)))\n",
        "\n",
        "# Start from your proposed weights\n",
        "start = {\"xgb_tweedie\":0.40,\"xgb_log1p\":0.25,\"lgbm_rmse\":0.25,\"lgbm_tweedie\":0.10}\n",
        "start = {m: start.get(m, 0.0) for m in models}\n",
        "s = sum(start.values());\n",
        "w_best = {k: (v/s if s>0 else 1.0/len(models)) for k,v in start.items()}\n",
        "\n",
        "def yhat_from_w(w):\n",
        "    yh = 0.0\n",
        "    for m in models: yh += w[m]*o[m]\n",
        "    return yh\n",
        "\n",
        "best = (w_best, smape_np(o['y'], yhat_from_w(w_best)))\n",
        "print(\"Start sMAPE:\", best[1])\n",
        "\n",
        "# Coordinate search around start (non-negative, sum=1)\n",
        "grid = np.linspace(-0.10, 0.10, 21)\n",
        "for m in models:\n",
        "    base = w_best.copy()\n",
        "    for delta in grid:\n",
        "        w_try = base.copy()\n",
        "        w_try[m] = max(0.0, w_try[m] + delta)\n",
        "        s = sum(w_try.values())\n",
        "        if s <= 0: continue\n",
        "        w_try = {k: v/s for k, v in w_try.items()}\n",
        "        score = smape_np(o['y'], yhat_from_w(w_try))\n",
        "        if score < best[1]:\n",
        "            best = (w_try, score)\n",
        "\n",
        "w_opt, smape_opt = best\n",
        "print(\"Optimal weights:\", w_opt, \"| OOF sMAPE:\", smape_opt)\n",
        "\n",
        "# Save weights + provenance\n",
        "LOG_DIR = \"/content/drive/MyDrive/ai4trade/logs\"\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "with open(f\"{LOG_DIR}/ensemble_weights_trees_h2.json\",\"w\") as f:\n",
        "    json.dump({\"weights\": w_opt, \"smape_oof\": smape_opt, \"files_used\": paths}, f, indent=2)\n",
        "print(\"Saved:\", f\"{LOG_DIR}/ensemble_weights_trees_h2.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Apply optimized weights to forecasts (trees only) ===\n",
        "import os, json, glob, re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/ai4trade\"\n",
        "PRED_DIR = f\"{BASE_DIR}/predictions\"\n",
        "FC_DIR   = f\"{PRED_DIR}/forecast\"\n",
        "OUT_DIR  = f\"{PRED_DIR}/forecast_ensemble\"\n",
        "LOG_DIR  = f\"{BASE_DIR}/logs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Load weights you just saved\n",
        "with open(f\"{LOG_DIR}/ensemble_weights_trees_h2.json\") as f:\n",
        "    meta = json.load(f)\n",
        "w_opt = {k: float(v) for k, v in meta[\"weights\"].items()}\n",
        "models = list(w_opt.keys())\n",
        "print(\"Using weights:\", w_opt)\n",
        "\n",
        "# --- Robust file finder ---\n",
        "def find_parquet_for(model_key, search_dir):\n",
        "    # Prefer files containing model_key and not 'oof'\n",
        "    cands = [p for p in glob.glob(os.path.join(search_dir, \"*.parquet\"))\n",
        "             if model_key in os.path.basename(p).lower() and \"oof\" not in os.path.basename(p).lower()]\n",
        "    if not cands:\n",
        "        # fallback: allow 'oof' if nothing else (but we *should* have forecast files)\n",
        "        cands = [p for p in glob.glob(os.path.join(search_dir, \"*.parquet\"))\n",
        "                 if model_key in os.path.basename(p).lower()]\n",
        "    if not cands:\n",
        "        raise FileNotFoundError(f\"No forecast parquet found for '{model_key}' in {search_dir}\")\n",
        "    # If multiple, pick the most recent\n",
        "    cands.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
        "    return cands[0]\n",
        "\n",
        "# --- Column detection helpers ---\n",
        "KEYS = ['origin','destination','hs6','trade_flow','month']\n",
        "PRED_CANDS = ['y_pred', 'prediction', 'pred', 'yhat', 'y_pred_mean']\n",
        "\n",
        "def detect_pred_col(df, model_key):\n",
        "    exact = f\"y_pred_{model_key}\"\n",
        "    if exact in df.columns:\n",
        "        return exact\n",
        "    for c in PRED_CANDS:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    # any column starting with y_pred*\n",
        "    for c in df.columns:\n",
        "        if str(c).startswith(\"y_pred\"):\n",
        "            return c\n",
        "    # any column containing model key\n",
        "    for c in df.columns:\n",
        "        if model_key in str(c).lower():\n",
        "            return c\n",
        "    raise ValueError(f\"Could not find prediction column in columns: {df.columns.tolist()}\")\n",
        "\n",
        "def load_fc_detect(name, path):\n",
        "    df = pd.read_parquet(path)\n",
        "    missing = set(KEYS) - set(df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"{name}: missing keys {sorted(missing)} in {path}\")\n",
        "    pcol = detect_pred_col(df, name)\n",
        "    df = df[KEYS + [pcol]].rename(columns={pcol: name})\n",
        "    return df\n",
        "\n",
        "# --- Discover & load forecasts for each model by filename substring keys ---\n",
        "# Map model names to filename substrings to search\n",
        "model_key_to_substr = {\n",
        "    \"xgb_tweedie\":  \"xgb_tweedie\",\n",
        "    \"xgb_log1p\":    \"xgb_log1p\",\n",
        "    \"lgbm_rmse\":    \"lgbm_rmse\",\n",
        "    \"lgbm_tweedie\": \"lgbm_tweedie\",\n",
        "}\n",
        "\n",
        "file_map = {}\n",
        "for m in models:\n",
        "    fn = find_parquet_for(model_key_to_substr[m], FC_DIR)\n",
        "    file_map[m] = fn\n",
        "print(\"Forecast files found:\", file_map)\n",
        "\n",
        "# Load and inner-join\n",
        "fc = None\n",
        "for m in models:\n",
        "    dfm = load_fc_detect(m, file_map[m])\n",
        "    fc = dfm if fc is None else fc.merge(dfm, on=KEYS, how=\"inner\")\n",
        "\n",
        "# Blend\n",
        "fc['y_pred_ens'] = 0.0\n",
        "for m, w in w_opt.items():\n",
        "    fc['y_pred_ens'] = fc['y_pred_ens'] + w * fc[m]\n",
        "\n",
        "# Post-processing: non-negativity\n",
        "fc['y_pred_ens'] = fc['y_pred_ens'].clip(lower=0)\n",
        "\n",
        "# Save HS-6 ensemble\n",
        "hs6_out = fc[KEYS + ['y_pred_ens']].copy()\n",
        "hs6_path = f\"{OUT_DIR}/ensemble_h2_hs6.parquet\"\n",
        "hs6_out.to_parquet(hs6_path, index=False)\n",
        "print(\"Saved HS-6 ensemble:\", hs6_path)\n",
        "\n",
        "# Derive hs4 and aggregate\n",
        "if hs6_out['hs6'].dtype != 'O':\n",
        "    hs6_out['hs6'] = hs6_out['hs6'].astype(str)\n",
        "hs6_out['hs4'] = hs6_out['hs6'].str[:4]\n",
        "\n",
        "hs4_out = (hs6_out\n",
        "           .groupby(['origin','destination','hs4','trade_flow','month'], as_index=False)['y_pred_ens']\n",
        "           .sum())\n",
        "\n",
        "hs4_path = f\"{OUT_DIR}/ensemble_h2_hs4.parquet\"\n",
        "hs4_out.to_parquet(hs4_path, index=False)\n",
        "print(\"Saved HS-4 ensemble:\", hs4_path)\n",
        "\n",
        "# Competition-style CSV: “USA”, “CHL”, “8404”, “Export”, “1234567”\n",
        "csv_path = f\"{OUT_DIR}/submission_h2_hs4.csv\"\n",
        "sub = hs4_out.rename(columns={'y_pred_ens':'value'})\n",
        "sub = sub[['origin','destination','hs4','trade_flow','month','value']]\n",
        "sub.to_csv(csv_path, index=False)\n",
        "print(\"Saved submission CSV:\", csv_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDEZlZO0Uhvl",
        "outputId": "1847d03f-1a0b-433a-9836-8451c857ea63"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using weights: {'xgb_tweedie': 0.4444444444444445, 'xgb_log1p': 0.16666666666666666, 'lgbm_rmse': 0.2777777777777778, 'lgbm_tweedie': 0.11111111111111112}\n",
            "Forecast files found: {'xgb_tweedie': '/content/drive/MyDrive/ai4trade/predictions/forecast/xgb_tweedie_forecast_xgb_tweedie_h2_20251024_1438_p1.5.parquet', 'xgb_log1p': '/content/drive/MyDrive/ai4trade/predictions/forecast/xgb_log1p_forecast.parquet', 'lgbm_rmse': '/content/drive/MyDrive/ai4trade/predictions/forecast/lgbm_rmse_forecast.parquet', 'lgbm_tweedie': '/content/drive/MyDrive/ai4trade/predictions/forecast/lgbm_tweedie_forecast_run_h2_20251023_165145.parquet'}\n",
            "Saved HS-6 ensemble: /content/drive/MyDrive/ai4trade/predictions/forecast_ensemble/ensemble_h2_hs6.parquet\n",
            "Saved HS-4 ensemble: /content/drive/MyDrive/ai4trade/predictions/forecast_ensemble/ensemble_h2_hs4.parquet\n",
            "Saved submission CSV: /content/drive/MyDrive/ai4trade/predictions/forecast_ensemble/submission_h2_hs4.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "ens = pd.read_parquet(\"/content/drive/MyDrive/ai4trade/predictions/forecast_ensemble/ensemble_h2_hs6.parquet\")\n",
        "print(sorted(ens['month'].unique()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv2bRjYDV5Fc",
        "outputId": "72b921a2-638e-4140-c4cd-404dfcc9a94a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Timestamp('2024-08-01 00:00:00')]\n"
          ]
        }
      ]
    }
  ]
}